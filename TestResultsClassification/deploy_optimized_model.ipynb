{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2650cb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/cpan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import nltk\n",
    "import torch\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "461d5863",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb = joblib.load('/Users/cpan/Desktop/project/notebooks/tuned_xgb_model1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6e60e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = '/Users/cpan/OneDrive - Guardant Health/clean_test_results/labeled_test_results.json'\n",
    "with open(json_file_path, 'r') as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a07febf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(data):\n",
    "    dataframe = []\n",
    "    for entry in data:\n",
    "        file_name = entry['file_name']\n",
    "        patient_id = entry['patient_id']\n",
    "        page_content = entry['page_content']\n",
    "        tests = entry['tests']    \n",
    "        dataframe.append({'file_name': file_name, 'patient_id': patient_id, 'page_content': page_content})\n",
    "    df = pd.DataFrame(dataframe)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3512185",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = create_dataframe(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34b7ebd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2793, 3)\n"
     ]
    }
   ],
   "source": [
    "print(df_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5965143e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Initialize the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0744b03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters, digits, and extra spaces\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Define your custom vocabulary to be added to stopwords\n",
    "    custom_vocab = [\"redacted\", \"redactedredacted\", 'is', 'to', 'of', 'with', 'in', 'no']  # Add your domain-specific terms\n",
    "    \n",
    "    # Combine NLTK stopwords and custom vocabulary\n",
    "    stop_words = set(stopwords.words('english') + custom_vocab)\n",
    "    \n",
    "    words = text.split()\n",
    "    # filter out words that have length less than 2 \n",
    "    filtered_words = [word for word in words if len(word) > 2 and word not in stop_words]\n",
    "    text = ' '.join(filtered_words)\n",
    "    \n",
    "    return text\n",
    "# Function to extract BERT embeddings for text data\n",
    "def extract_bert_features(df):\n",
    "    embeddings = []\n",
    "    for text in df['page_content']:\n",
    "        cleaned_text = clean_text(text)\n",
    "        inputs = tokenizer(cleaned_text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "        avg_pooling = torch.mean(last_hidden_states, dim=1)\n",
    "        embeddings.append(avg_pooling.squeeze().numpy())\n",
    "    return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bc121b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = extract_bert_features(df_data)\n",
    "X_test = test_features\n",
    "scaler = StandardScaler()\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "y_pred_tuned = best_xgb_tuned.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5c89cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['is_test'] = y_pred_tuned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a15c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export prediction dataframe to a csv file \n",
    "save_dir = '/Users/cpan/OneDrive - Guardant Health/page_classification_data/'\n",
    "train_df.to_csv(save_dir+\"predictions_first3000_docs.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
